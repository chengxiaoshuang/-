#单隐层神经网络
#定义网络结构
#假设X为神经网络的输入特征矩阵，Y为标签向量

import numpy as np

def layer_size(X,Y):
    n_x=X.shape[0] # size of input layer
    n_h=4 #size of hidden layer
    n_y=Y.shape[0] #size of output layer
    return (n_x,n_h,n_y)
#其中输入层和输出层的大小分别与 X 和 y 的 shape 有关。
#而隐层的大小可由我们手动指定。这里我们指定隐层的大小为4
#初始化模型参数
#假设 W1 为输入层到隐层的权重数组、b1 为输入层到隐层的偏置数组；W2 为隐层到输出层的权重数组，b2 为隐层到输出层的偏置数组

def initialize_parameters(n_x,n_h,n_y):
    W1=np.random.randn(n_h,n_x)*0.01
    b1=np.zeros((n_h,1))
    W2=np.random.randn(n_y,n_h)*0.01
    b2=np.zeros((n_y,1))
    
    assert (W1.shape == (n_h, n_x))    
    assert (b1.shape == (n_h, 1))    
    assert (W2.shape == (n_y, n_h))    
    assert (b2.shape == (n_y, 1))
    
    parameters={'W1':W1,'b1':b1,'W2':W2,'b2':b2}
    
    return parameters
前向传播
假设隐层的激活函数为 tanh 函数， 输出层的激活函数为 sigmoid 函数。

def forward_propagation(X,parameters):
    W1=parameters['W1']
    b1=parameters['b1']
    W2=parameters['W2']
    b2=parameters['b2']
    
    Z1=np.dot(W1,X)+b1
    A1=np.tanh(Z1)
    Z2=np.dot(W2,Z1)+b2
    A2=sigmoid(Z2)
    assert(A2.shape==(1,X.shape[1]))
    
    cache={'Z1':Z1,'A1':A1,'Z2':Z2,'A2':A2}
    
    return A2,cache
#计算当前训练损失
#损失函数为交叉熵

def comput_cost(A2,Y,parameters):
    m=Y.shape[1]
    logprobs=np.multiply(np.log(A2,Y)+np.multiply(np.log(1-A2),1-Y))
    cost=-1/m *np.sum(logprobs)
    cost=np.squeeze(cost)
    assert (isinstance(cost,float))
    return cost
#执行反向传播

def backward_propagation(parameters,cache,X,Y):
    m=X.shape[1]
    W1=parameters['W1']
    W2=parameters['W2']
    A1=cache['A1']
    A2=cache['A2']
    
    dz2=A2-Y
    dw2=1/m*np.dot(dz2,A1.T)
    db2=1/m * np.sum(dz2,axis=1,keepdims=True)
    dz1=np.dot(W2.T,dz2)*(1-np.poewr(A1,2))
    dw1=1/m * np.dot(dz1,X.T)
    db1=1/m *np.sum(dz1,axis=1,keepdims=True)
    
    grads={'dw1':dw1,"dw2":dw2,"db1":db1,"db2":db2}
    
    return grads
#权值更新

def updata_parameters(parameters,grads,learning_rate=1.2):
    W1=parameters['W1']
    b1=parameters['b1']
    W2=parameters['W2']
    b2=parameters['b2']
    
    dw1=grads['dw1']
    dw2=grads['dw2']
    db1=grads['db1']
    db2=grads['db2']
    
    W1 -= dw1 *learning_rate
    b1 -= db1 *learning_rate
    W2 -= dw2 *learning_rate
    b2 -= db2 *learning_rate
    
    parameters={'W1':W1,'b1':b1,'W2':W2,'b2':b2}
    
    return parameters
#定义一个神经网络

def nn_model(X,Y,n_h,num_iterations=10000,print_cost=False):
    np.random.seed(3)
    n_x=layer_sizes(X,Y)[0]
    n_y=layer_sizes(X,Y)[2]
    
    parameters=initialize_parameters(n_x,n_h,n_y)
    w1=parameters['W1']
    b1=parameters['b1']
    W2=parameters['W2']
    b2=parameters['b2']
    
    for i in range(0,num_iterations):
        # Forward propagation. Inputs: "X, parameters". Outputs: "A2, cache".
        A2,cache=forward_propagation(X,parameters)
        # Cost function. Inputs: "A2, Y, parameters". Outputs: "cost".
        cost=comput_cost(A2,Y,parameters)
        # Backpropagation. Inputs: "parameters, cache, X, Y". Outputs: "grads".
        grads=backward_propagation(parameters,cache,X,Y)
        # Gradient descent parameter update. Inputs: "parameters, grads". Outputs: "parameters".
        parameters=updata_parameters(parameters,grads,learning_rate=1.2)
        
        if print_cost and i%1000==0:
            print ('Cost after iteration %i: %f' %(i,cost))
            
        return parameters
        

​

