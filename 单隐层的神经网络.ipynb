{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 单隐层神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义网络结构\n",
    "#### 假设X为神经网络的输入特征矩阵，Y为标签向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_size(X,Y):\n",
    "    n_x=X.shape[0] # size of input layer\n",
    "    n_h=4 #size of hidden layer\n",
    "    n_y=Y.shape[0] #size of output layer\n",
    "    return (n_x,n_h,n_y)\n",
    "#其中输入层和输出层的大小分别与 X 和 y 的 shape 有关。\n",
    "#而隐层的大小可由我们手动指定。这里我们指定隐层的大小为4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化模型参数\n",
    "#### 假设 W1 为输入层到隐层的权重数组、b1 为输入层到隐层的偏置数组；W2 为隐层到输出层的权重数组，b2 为隐层到输出层的偏置数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x,n_h,n_y):\n",
    "    W1=np.random.randn(n_h,n_x)*0.01\n",
    "    b1=np.zeros((n_h,1))\n",
    "    W2=np.random.randn(n_y,n_h)*0.01\n",
    "    b2=np.zeros((n_y,1))\n",
    "    \n",
    "    assert (W1.shape == (n_h, n_x))    \n",
    "    assert (b1.shape == (n_h, 1))    \n",
    "    assert (W2.shape == (n_y, n_h))    \n",
    "    assert (b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters={'W1':W1,'b1':b1,'W2':W2,'b2':b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前向传播\n",
    "#### 假设隐层的激活函数为 tanh 函数， 输出层的激活函数为 sigmoid 函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X,parameters):\n",
    "    W1=parameters['W1']\n",
    "    b1=parameters['b1']\n",
    "    W2=parameters['W2']\n",
    "    b2=parameters['b2']\n",
    "    \n",
    "    Z1=np.dot(W1,X)+b1\n",
    "    A1=np.tanh(Z1)\n",
    "    Z2=np.dot(W2,Z1)+b2\n",
    "    A2=sigmoid(Z2)\n",
    "    assert(A2.shape==(1,X.shape[1]))\n",
    "    \n",
    "    cache={'Z1':Z1,'A1':A1,'Z2':Z2,'A2':A2}\n",
    "    \n",
    "    return A2,cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算当前训练损失\n",
    "#### 损失函数为交叉熵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comput_cost(A2,Y,parameters):\n",
    "    m=Y.shape[1]\n",
    "    logprobs=np.multiply(np.log(A2,Y)+np.multiply(np.log(1-A2),1-Y))\n",
    "    cost=-1/m *np.sum(logprobs)\n",
    "    cost=np.squeeze(cost)\n",
    "    assert (isinstance(cost,float))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  执行反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters,cache,X,Y):\n",
    "    m=X.shape[1]\n",
    "    W1=parameters['W1']\n",
    "    W2=parameters['W2']\n",
    "    A1=cache['A1']\n",
    "    A2=cache['A2']\n",
    "    \n",
    "    dz2=A2-Y\n",
    "    dw2=1/m*np.dot(dz2,A1.T)\n",
    "    db2=1/m * np.sum(dz2,axis=1,keepdims=True)\n",
    "    dz1=np.dot(W2.T,dz2)*(1-np.poewr(A1,2))\n",
    "    dw1=1/m * np.dot(dz1,X.T)\n",
    "    db1=1/m *np.sum(dz1,axis=1,keepdims=True)\n",
    "    \n",
    "    grads={'dw1':dw1,\"dw2\":dw2,\"db1\":db1,\"db2\":db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 权值更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updata_parameters(parameters,grads,learning_rate=1.2):\n",
    "    W1=parameters['W1']\n",
    "    b1=parameters['b1']\n",
    "    W2=parameters['W2']\n",
    "    b2=parameters['b2']\n",
    "    \n",
    "    dw1=grads['dw1']\n",
    "    dw2=grads['dw2']\n",
    "    db1=grads['db1']\n",
    "    db2=grads['db2']\n",
    "    \n",
    "    W1 -= dw1 *learning_rate\n",
    "    b1 -= db1 *learning_rate\n",
    "    W2 -= dw2 *learning_rate\n",
    "    b2 -= db2 *learning_rate\n",
    "    \n",
    "    parameters={'W1':W1,'b1':b1,'W2':W2,'b2':b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义一个神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X,Y,n_h,num_iterations=10000,print_cost=False):\n",
    "    np.random.seed(3)\n",
    "    n_x=layer_sizes(X,Y)[0]\n",
    "    n_y=layer_sizes(X,Y)[2]\n",
    "    \n",
    "    parameters=initialize_parameters(n_x,n_h,n_y)\n",
    "    w1=parameters['W1']\n",
    "    b1=parameters['b1']\n",
    "    W2=parameters['W2']\n",
    "    b2=parameters['b2']\n",
    "    \n",
    "    for i in range(0,num_iterations):\n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "        A2,cache=forward_propagation(X,parameters)\n",
    "        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
    "        cost=comput_cost(A2,Y,parameters)\n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        grads=backward_propagation(parameters,cache,X,Y)\n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        parameters=updata_parameters(parameters,grads,learning_rate=1.2)\n",
    "        \n",
    "        if print_cost and i%1000==0:\n",
    "            print ('Cost after iteration %i: %f' %(i,cost))\n",
    "            \n",
    "        return parameters\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
